{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "import mapply\n",
    "mapply.init(n_workers=7, progressbar=True)\n",
    "\n",
    "import pickle\n",
    "with open('scaler.pickle', 'rb') as f:\n",
    "    scaler: StandardScaler = pickle.load(f)\n",
    "\n",
    "# Read in the data\n",
    "final = pd.read_csv('../dataset/data/final_clean.csv')\n",
    "\n",
    "# Remember to the program which ðŸ¦†ing columns are objects\n",
    "cat_cols = ['date_month', 'date_day', 'date_year', 'congressional_district', 'state', 'incident_characteristics1', 'city_or_county', 'party', 'CLEAN', 'OUTLIER']\n",
    "final[cat_cols] = final[cat_cols].astype('object')\n",
    "\n",
    "\n",
    "\n",
    "final['isArrested'] = (final['n_arrested'] > 0)\n",
    "final['isInjured'] = (final['n_injured'] > 0)\n",
    "final['isUnharmed'] = (final['n_unharmed'] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "discretized = final.copy()\n",
    "\n",
    "TO_DISCRETIZE = ['state', 'congressional_district', 'party']\n",
    "\n",
    "cat_cols = final.select_dtypes(include=['object']).columns\n",
    "def discretize_data(dataset, variables):\n",
    "    for variable in variables:\n",
    "        #get the unique variable's values\n",
    "        var = sorted(dataset[variable].unique())\n",
    "        \n",
    "        #generate a mapping from the variable's values to the number representation  \n",
    "        mapping = dict(zip(var, range(0, len(var) + 1)))\n",
    "\n",
    "        #add a new colum with the number representation of the variable\n",
    "        dataset[variable+'_num'] = dataset[variable].map(mapping).astype(int)\n",
    "    return dataset\n",
    "\n",
    "def one_hot(dataset, variables):\n",
    "    for variable in variables:\n",
    "        #get the unique variable's values\n",
    "        vars = sorted(dataset[variable].unique())\n",
    "        \n",
    "        for var in vars:\n",
    "            dataset[variable+'_'+str(var)] = (dataset[variable] == var).astype(int)\n",
    "    return dataset\n",
    "\n",
    "# discretized = one_hot(discretized, TO_DISCRETIZE)\n",
    "discretized = discretized.drop(columns=cat_cols)\n",
    "\n",
    "# final = discretize_data(final, cat_cols)\n",
    "# final = final.drop(columns=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the data\n",
    "scaled = scaler.fit_transform(discretized)\n",
    "\n",
    "# Convert to a dataframe\n",
    "scaled = pd.DataFrame(scaled, columns=discretized.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n_killed', 'p_killed', 'month_cd_ratio_killed', 'n_injured', 'p_injured', 'month_cd_ratio_injured', 'n_arrested', 'p_arrested', 'month_cd_ratio_arrested', 'n_unharmed', 'p_unharmed', 'month_cd_ratio_unharmed', 'n_participants_child', 'n_participants_teen', 'n_participants_adult', 'n_participants', 'cd_month_SHOT_DEAD']\n"
     ]
    }
   ],
   "source": [
    "f = scaled.copy()\n",
    "\n",
    "f['isKilled'] = (f['n_killed'] > 0)\n",
    "\n",
    "value_counts = f['isKilled'].value_counts()\n",
    "min_value_count = value_counts.min()\n",
    "f = f.groupby('isKilled').apply(lambda x: x.sample(n=min_value_count))\n",
    "\n",
    "isKilled = (f['n_killed'] > 0).astype('int64')\n",
    "\n",
    "del f['isKilled']\n",
    "\n",
    "keywords = ['killed', 'injured', 'arrested', 'unharmed', 'n_participants', 'DEAD', 'CLEAN', 'OUTLIER', 'incident_']\n",
    "# del all columns with keywords in it\n",
    "deleted = []\n",
    "for keyword in keywords:\n",
    "    for col in f.columns:\n",
    "        if keyword in col:\n",
    "            deleted.append(col)\n",
    "            del f[col]\n",
    "\n",
    "print(deleted)\n",
    "\n",
    "f = f.astype('float64')\n",
    "\n",
    "# del f['n_killed']\n",
    "# del f['p_killed']\n",
    "# del f['month_cd_ratio_killed']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(f.select_dtypes(include=['float64', 'int64']), isKilled, stratify=isKilled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8707908120865605\n",
      "[[12285  2757]\n",
      " [ 1130 13911]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=6, min_samples_split=3, min_samples_leaf=4)\n",
    "# clf = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=500, alpha=0.0001, solver='adam', verbose=10,  random_state=21,tol=0.000000001)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "import pydotplus\n",
    "\n",
    "# from IPython.display import Image\n",
    "dot_data = export_graphviz(clf, out_file=None, feature_names=f.select_dtypes(include=['float64', 'int64']).columns, class_names=['Not_Killed', 'Killed'], filled=True, rounded=True, special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "\n",
    "graph.write_png('graph.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000, Loss: 514871.3750\n",
      "Epoch 101/5000, Loss: 511638.2500\n",
      "Epoch 201/5000, Loss: 511620.1250\n",
      "Epoch 301/5000, Loss: 511533.2188\n",
      "Epoch 401/5000, Loss: 511499.7812\n",
      "Epoch 501/5000, Loss: 511512.6250\n",
      "Epoch 601/5000, Loss: 511466.0938\n",
      "Epoch 701/5000, Loss: 511493.5000\n",
      "Epoch 801/5000, Loss: 511434.5000\n",
      "Epoch 901/5000, Loss: 511447.6250\n",
      "Epoch 1001/5000, Loss: 511470.9688\n",
      "Epoch 1101/5000, Loss: 511420.8125\n",
      "Epoch 1201/5000, Loss: 511482.2500\n",
      "Epoch 1301/5000, Loss: 511404.8750\n",
      "Epoch 1401/5000, Loss: 511423.0938\n",
      "Epoch 1501/5000, Loss: 511417.6250\n",
      "Epoch 1601/5000, Loss: 511391.2500\n",
      "Epoch 1701/5000, Loss: 511430.5625\n",
      "Epoch 1801/5000, Loss: 511350.3750\n",
      "Epoch 1901/5000, Loss: 511319.0625\n",
      "Epoch 2001/5000, Loss: 511250.5312\n",
      "Epoch 2101/5000, Loss: 511225.1250\n",
      "Epoch 2201/5000, Loss: 511192.9375\n",
      "Epoch 2301/5000, Loss: 511200.9375\n",
      "Epoch 2401/5000, Loss: 511246.8125\n",
      "Epoch 2501/5000, Loss: 511200.4062\n",
      "Epoch 2601/5000, Loss: 511163.5625\n",
      "Epoch 2701/5000, Loss: 511195.0625\n",
      "Epoch 2801/5000, Loss: 511161.4375\n",
      "Epoch 2901/5000, Loss: 511131.0000\n",
      "Epoch 3001/5000, Loss: 511136.9688\n",
      "Epoch 3101/5000, Loss: 511143.9062\n",
      "Epoch 3201/5000, Loss: 511163.8750\n",
      "Epoch 3301/5000, Loss: 511320.0000\n",
      "Epoch 3401/5000, Loss: 511116.3750\n",
      "Epoch 3501/5000, Loss: 511277.6562\n",
      "Epoch 3601/5000, Loss: 511113.9688\n",
      "Epoch 3701/5000, Loss: 511128.4688\n",
      "Epoch 3801/5000, Loss: 511113.5312\n",
      "Epoch 3901/5000, Loss: 511260.5312\n",
      "Epoch 4001/5000, Loss: 511132.3125\n",
      "Epoch 4101/5000, Loss: 511122.6250\n",
      "Epoch 4201/5000, Loss: 511104.4375\n",
      "Epoch 4301/5000, Loss: 511103.0312\n",
      "Epoch 4401/5000, Loss: 511111.8125\n",
      "Epoch 4501/5000, Loss: 511146.8125\n",
      "Epoch 4601/5000, Loss: 511137.1562\n",
      "Epoch 4701/5000, Loss: 511099.5625\n",
      "Epoch 4801/5000, Loss: 511040.8438\n",
      "Epoch 4901/5000, Loss: 511134.6250\n",
      "[[ 8357  6684]\n",
      " [ 3425 11617]]\n",
      "0.6639630356015025\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(DEVICE)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(DEVICE)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(DEVICE)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, output_size),\n",
    "    nn.Sigmoid()\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "EPOCHS = 5000\n",
    "losses = []\n",
    "epochs = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_train_tensor)\n",
    "    y_pred = y_pred.squeeze(1)\n",
    "    loss = loss_function(y_pred, y_train_tensor)\n",
    "    losses.append(loss.item())\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {loss.item():.4f}')\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Convert the test data to PyTorch tensors and get the predicted class\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(DEVICE)\n",
    "y_pred_tensor = model(X_test_tensor).detach().cpu()\n",
    "\n",
    "y_pred = np.where(y_pred_tensor > 0.5, 1, 0)\n",
    "\n",
    "y_pred\n",
    "\n",
    "# _, y_pred = torch.max(y_pred_tensor, 1)\n",
    "\n",
    "# # Print the confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# # Print the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6257  8784]\n",
      " [ 3573 11469]]\n",
      "0.5892364458331948\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mapply\n",
    "mapply.init(n_workers=7, progressbar=True)\n",
    "\n",
    "# Read in the data\n",
    "final = pd.read_csv('../dataset/data/final_clean3.csv')\n",
    "\n",
    "# final['isArrested'] = (final['n_arrested'] > 0)\n",
    "# final['isInjured'] = (final['n_injured'] > 0)\n",
    "# final['isUnharmed'] = (final['n_unharmed'] > 0)\n",
    "# final = final[final['OUTLIER'] == 0]\n",
    "\n",
    "final['isKilled'] = (final['n_killed'] > 0)\n",
    "final['isInjured'] = (final['n_injured'] > 0)\n",
    "final['isUnharmed'] = (final['n_unharmed'] > 0)\n",
    "final['isArrested'] = (final['n_arrested'] > 0)\n",
    "\n",
    "final['congressional_district'] = final['congressional_district'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented = final.copy()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_valid, test = train_test_split(augmented, test_size=0.2)\n",
    "train, valid = train_test_split(train_valid, test_size=0.2)\n",
    "\n",
    "train: pd.DataFrame = train\n",
    "valid: pd.DataFrame = valid\n",
    "test: pd.DataFrame = test\n",
    "\n",
    "idx_train = train.index\n",
    "idx_valid = valid.index\n",
    "idx_test = test.index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train['state_year_p_isKilled'] = train.groupby(['state', 'date_year', 'date_month'])['isKilled'].transform('mean')\n",
    "train['state_year_p_isArrested'] = train.groupby(['state', 'date_year', 'date_month'])['isArrested'].transform('mean')\n",
    "train['state_year_p_isInjured'] = train.groupby(['state', 'date_year', 'date_month'])['isInjured'].transform('mean')\n",
    "train['state_year_p_isUnharmed'] = train.groupby(['state', 'date_year', 'date_month'])['isUnharmed'].transform('mean')\n",
    "\n",
    "# print([x for x in train[['state', 'date_year', 'state_year_p_isKilled']].drop_duplicates().values])\n",
    "\n",
    "state_year_p_isKilled = {(x[0], x[1], x[2]): (x[3], x[4], x[5], x[6]) for x in train[['state', 'date_year', 'date_month', 'state_year_p_isKilled', 'state_year_p_isArrested', 'state_year_p_isInjured', 'state_year_p_isUnharmed']].drop_duplicates().values}\n",
    "def state_year_p_isKilled_lookup(state, year, month, idx):\n",
    "    try:\n",
    "        return state_year_p_isKilled[(state, year, month)][idx]\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "valid['state_year_p_isKilled'] = valid.apply(lambda x: state_year_p_isKilled_lookup(x['state'], x['date_year'], x['date_month'], 0), axis=1)\n",
    "valid['state_year_p_isArrested'] = valid.apply(lambda x: state_year_p_isKilled_lookup(x['state'], x['date_year'], x['date_month'], 1), axis=1)\n",
    "valid['state_year_p_isInjured'] = valid.apply(lambda x: state_year_p_isKilled_lookup(x['state'], x['date_year'], x['date_month'], 2), axis=1)\n",
    "valid['state_year_p_isUnharmed'] = valid.apply(lambda x: state_year_p_isKilled_lookup(x['state'], x['date_year'], x['date_month'], 3), axis=1)\n",
    "\n",
    "test['state_year_p_isKilled'] = test.apply(lambda x: state_year_p_isKilled_lookup(x['state'], x['date_year'], x['date_month'], 0), axis=1)\n",
    "test['state_year_p_isArrested'] = test.apply(lambda x: state_year_p_isKilled_lookup(x['state'], x['date_year'], x['date_month'], 1), axis=1)\n",
    "test['state_year_p_isInjured'] = test.apply(lambda x: state_year_p_isKilled_lookup(x['state'], x['date_year'], x['date_month'], 2), axis=1)\n",
    "test['state_year_p_isUnharmed'] = test.apply(lambda x: state_year_p_isKilled_lookup(x['state'], x['date_year'], x['date_month'], 3), axis=1)\n",
    "\n",
    "# augmented.loc[idx_train, 'state_year_p_isKilled'] = train['state_year_p_isKilled']\n",
    "# augmented.loc[idx_train, 'state_year_p_isArrested'] = train['state_year_p_isArrested']\n",
    "# augmented.loc[idx_train, 'state_year_p_isInjured'] = train['state_year_p_isInjured']\n",
    "# augmented.loc[idx_train, 'state_year_p_isUnharmed'] = train['state_year_p_isUnharmed']\n",
    "# augmented.loc[idx_valid, 'state_year_p_isKilled'] = valid['state_year_p_isKilled']\n",
    "# augmented.loc[idx_valid, 'state_year_p_isArrested'] = valid['state_year_p_isArrested']\n",
    "# augmented.loc[idx_valid, 'state_year_p_isInjured'] = valid['state_year_p_isInjured']\n",
    "# augmented.loc[idx_valid, 'state_year_p_isUnharmed'] = valid['state_year_p_isUnharmed']\n",
    "# augmented.loc[idx_test, 'state_year_p_isKilled'] = test['state_year_p_isKilled']\n",
    "# augmented.loc[idx_test, 'state_year_p_isArrested'] = test['state_year_p_isArrested']\n",
    "# augmented.loc[idx_test, 'state_year_p_isInjured'] = test['state_year_p_isInjured']\n",
    "# augmented.loc[idx_test, 'state_year_p_isUnharmed'] = test['state_year_p_isUnharmed']\n",
    "\n",
    "augmented['week'] = augmented['timestamp'].floordiv(7)\n",
    "augmented.set_index(['city_or_county', 'week'], inplace=True)\n",
    "augmented['n_incidents_city_week'] = augmented.groupby(augmented.index.names).size()\n",
    "augmented.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235219, 133)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discretized = augmented.copy()\n",
    "\n",
    "discretized['state_congressional_district'] = discretized['state'].astype(str) + '_' + discretized['congressional_district'].astype(str)\n",
    "\n",
    "TO_DISCRETIZE = ['state', 'party']\n",
    "\n",
    "cat_cols = discretized.select_dtypes(include=['object']).columns\n",
    "def discretize_data(dataset, variables):\n",
    "    for variable in variables:\n",
    "        #get the unique variable's values\n",
    "        var = sorted(dataset[variable].unique())\n",
    "        \n",
    "        #generate a mapping from the variable's values to the number representation  \n",
    "        mapping = dict(zip(var, range(0, len(var) + 1)))\n",
    "\n",
    "        #add a new colum with the number representation of the variable\n",
    "        dataset[variable+'_num'] = dataset[variable].map(mapping).astype(int)\n",
    "    return dataset\n",
    "\n",
    "def one_hot(dataset, variables):\n",
    "    for variable in variables:\n",
    "        #get the unique variable's values\n",
    "        vars = sorted(dataset[variable].unique())\n",
    "        \n",
    "        for var in vars:\n",
    "            dataset[variable+'_'+str(var)] = (dataset[variable] == var).astype(int)\n",
    "    return dataset\n",
    "\n",
    "discretized = one_hot(discretized, TO_DISCRETIZE)\n",
    "# discretized = discretize_data(discretized, cat_cols)\n",
    "discretized = discretized.drop(columns=cat_cols)\n",
    "\n",
    "discretized.shape\n",
    "\n",
    "# final = final.drop(columns=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['week', 'latitude', 'longitude', 'avg_age_participants',\n",
      "       'max_age_participants', 'n_participants_child', 'n_participants_teen',\n",
      "       'n_injured', 'n_arrested', 'n_unharmed', 'timestamp',\n",
      "       'povertyPercentage', 'totalvotes', 'p_females', 'p_participants_child',\n",
      "       'p_participants_teen', 'month_cd_ratio_participants_child',\n",
      "       'month_cd_ratio_participants_teen',\n",
      "       'month_state_ratio_participants_child',\n",
      "       'month_state_ratio_participants_teen', 'percentage_republican',\n",
      "       'percentage_democrat', 'n_incidents_city_week', 'party_DEMOCRAT',\n",
      "       'party_REPUBLICAN'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "f = discretized.copy()\n",
    "\n",
    "isKilled = f['isKilled']\n",
    "\n",
    "del f['isKilled']\n",
    "del f['isInjured']\n",
    "del f['isUnharmed']\n",
    "del f['isArrested']\n",
    "\n",
    "# keywords = ['CLEAN', 'OUTLIER']\n",
    "# whitelist = ['participants_', 'state_year_p_']\n",
    "# blacklist = ['killed']\n",
    "keywords = ['n_', 'date', 'killed', 'arrested', 'injured', 'unharmed', '_males', 'n_participants', 'DEAD', 'CLEAN', 'OUTLIER', 'incident_', 'cd_', 'state_']\n",
    "whitelist = ['participants_', 'n_injured', 'n_unharmed', 'n_arrested', 'state_year_p_', 'n_incidents_city_week']\n",
    "blacklist = ['killed', 'adult']\n",
    "\n",
    "# del all columns with keywords in it\n",
    "deleted = []\n",
    "for col in f.columns:\n",
    "    if any([b in col for b in blacklist]):\n",
    "        deleted.append(col)\n",
    "        del f[col]\n",
    "    elif any([w in col for w in keywords]) and not any([w in col for w in whitelist]):\n",
    "        deleted.append(col)\n",
    "        del f[col]\n",
    "\n",
    "print(f.columns)\n",
    "\n",
    "# f = f[['povertyPercentage', 'date_year']]\n",
    "\n",
    "\n",
    "f = f.astype('float64')\n",
    "\n",
    "\n",
    "# del f['n_killed']\n",
    "# del f['p_killed']\n",
    "# del f['month_cd_ratio_killed']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler as S\n",
    "\n",
    "scaler = S()\n",
    "\n",
    "# Scale the data\n",
    "scaled = scaler.fit_transform(f)\n",
    "\n",
    "# Convert to a dataframe\n",
    "scaled = pd.DataFrame(scaled, columns=f.columns)\n",
    "\n",
    "scaled = scaled.drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# scaled = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "UPSAMPLE   = True\n",
    "DOWNSAMPLE = False\n",
    "\n",
    "scaled['isKilled'] = isKilled\n",
    "\n",
    "train = scaled[scaled.index.isin(idx_train)]\n",
    "valid = scaled[scaled.index.isin(idx_valid)]\n",
    "test = scaled[scaled.index.isin(idx_test)]\n",
    "\n",
    "# train = scaled.loc[idx_train]\n",
    "# valid = scaled.loc[idx_valid]\n",
    "# test = scaled.loc[idx_test]\n",
    "\n",
    "train_vc = train['isKilled'].value_counts()\n",
    "valid_vc = valid['isKilled'].value_counts()\n",
    "test_vc = test['isKilled'].value_counts()\n",
    "\n",
    "\n",
    "def rebalance(df, size):\n",
    "    if df.shape[0] < size:\n",
    "        return df.sample(size, replace=True, random_state=0)\n",
    "    else:\n",
    "        return df.sample(size, random_state=0)\n",
    "\n",
    "if UPSAMPLE:\n",
    "    max_train_vc = train_vc.max()\n",
    "    max_valid_vc = valid_vc.max()\n",
    "    max_test_vc = test_vc.max()\n",
    "    train = train.groupby('isKilled').apply(lambda x: rebalance(x, max_train_vc)).reset_index(drop=True)\n",
    "    valid = valid.groupby('isKilled').apply(lambda x: rebalance(x, max_valid_vc)).reset_index(drop=True)\n",
    "    test = test.groupby('isKilled').apply(lambda x: rebalance(x, max_test_vc)).reset_index(drop=True)\n",
    "elif DOWNSAMPLE:\n",
    "    min_train_vc = train_vc.min()\n",
    "    min_valid_vc = valid_vc.min()\n",
    "    min_test_vc = test_vc.min()\n",
    "    train = train.groupby('isKilled').apply(lambda x: rebalance(x, min_train_vc)).reset_index(drop=True)\n",
    "    valid = valid.groupby('isKilled').apply(lambda x: rebalance(x, min_valid_vc)).reset_index(drop=True)\n",
    "    test = test.groupby('isKilled').apply(lambda x: rebalance(x, min_test_vc)).reset_index(drop=True)\n",
    "\n",
    "\n",
    "X_train = train.drop(columns=['isKilled'])\n",
    "y_train = train['isKilled']\n",
    "\n",
    "X_valid = valid.drop(columns=['isKilled'])\n",
    "y_valid = valid['isKilled']\n",
    "\n",
    "X_test = test.drop(columns=['isKilled'])\n",
    "y_test = test['isKilled']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebalance classes with sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class BalancedDecisionTreeClassifier(DecisionTreeClassifier):\n",
    "\n",
    "    def rebalance(self, X, y):\n",
    "        max_vc = y.value_counts().max()\n",
    "        self.X_columns_ = X.columns\n",
    "        self.y_columns_ = y.name\n",
    "        Xy = pd.concat([X, y], axis=1)\n",
    "        Xy = Xy.groupby(y.name).apply(lambda x: \n",
    "                x.sample(max_vc, replace=True, random_state=0) if x.shape[0] < max_vc else x.sample(max_vc, random_state=0)\n",
    "        ).reset_index(drop=True)\n",
    "        X = Xy.drop(columns=[y.name])\n",
    "        y = Xy[y.name]\n",
    "        return X, y\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = self.rebalance(X, y)\n",
    "        return super().fit(X, y)\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        return super().transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# calculate class_weights\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Create the classifier\n",
    "clf = BalancedDecisionTreeClassifier(random_state=0)\n",
    "# clf = DecisionTreeClassifier(random_state=0, class_weight=class_weights)\n",
    "\n",
    "# Create the parameters list you wish to tune\n",
    "\n",
    "parameters = {\n",
    "    'max_depth': np.logspace(1, 6, 6, base=2).astype(int),\n",
    "    'min_samples_split': np.logspace(1, 5, 5, base=2).astype(int),\n",
    "    'min_samples_leaf': np.logspace(0, 5, 6, base=2).astype(int),\n",
    "}\n",
    "\n",
    "# Start the grid search with f1 score as scoring metric \n",
    "grid_obj = GridSearchCV(clf, parameters, scoring='f1', cv=3, verbose=3, n_jobs=-1)\n",
    "\n",
    "# Train the grid search object to find the optimal parameters using the training set\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = (clf.fit(X_train, y_train)).predict(X_valid)\n",
    "\n",
    "best_predictions = best_clf.predict(X_valid)\n",
    "\n",
    "# Report the before-and-afterscores\n",
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"F1 score on validation data: {:.4f}\".format(f1_score(y_valid, predictions)))\n",
    "print(\"F1-score on validation data: \\n{}\".format(classification_report(y_valid, predictions)))\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(\"Final F1 score on the validation data: {:.4f}\".format(f1_score(y_valid, best_predictions)))\n",
    "print(\"Final F1-score on the validation data: \\n{}\".format(classification_report(y_valid, best_predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "y_pred_test = best_clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy_score(y_test, y_pred_test)))\n",
    "print(\"Precision: {:.4f}\".format(precision_score(y_test, y_pred_test)))\n",
    "print(\"Recall: {:.4f}\".format(recall_score(y_test, y_pred_test)))\n",
    "print(\"F1: {:.4f}\".format(f1_score(y_test, y_pred_test)))\n",
    "print(\"ROC AUC: {:.4f}\".format(roc_auc_score(y_test, y_pred_test)))\n",
    "\n",
    "# roc curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "y_pred_train = best_clf.predict_proba(X_train)[:, 1]\n",
    "y_pred_valid = best_clf.predict_proba(X_valid)[:, 1]\n",
    "y_pred_test = best_clf.predict_proba(X_test)[:, 1]\n",
    "fprr, tprr, thresholdsr = roc_curve(y_train, y_pred_train)\n",
    "fprv, tprv, thresholdsv = roc_curve(y_valid, y_pred_valid)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_test)\n",
    "roc_aucr = auc(fprr, tprr)\n",
    "roc_aucv = auc(fprv, tprv)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(fprr, tprr, color='darkgreen', lw=2, label='ROC curve train (area = {:.4f})'.format(roc_aucr))\n",
    "plt.plot(fprv, tprv, color='darkred', lw=2, label='ROC curve valid (area = {:.4f})'.format(roc_aucv))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve test (area = {:.4f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1 - Specificity')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "estimators = []\n",
    "\n",
    "params = grid_fit.best_params_\n",
    "for k in range(grid_fit.n_splits_):\n",
    "    X_T, X_V, y_T, y_V = train_test_split(X_train, y_train, test_size=0.2)\n",
    "    clf = DecisionTreeClassifier(**params)\n",
    "    clf.set_params(random_state=0)\n",
    "    clf.fit(X_T, y_T)\n",
    "    print(f\"Fold {k}: {f1_score(y_V, clf.predict(X_V))}\")\n",
    "    estimators.append(('dt_' + str(k), clf))\n",
    "\n",
    "# Create the classifier\n",
    "clf = VotingClassifier(estimators=estimators, voting='hard')\n",
    "\n",
    "# Train the classifier on the training set\n",
    "print(f1_score(y_valid, clf.fit(X_train, y_train).predict(X_valid)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unoptimized model on training data\n",
    "unop_train_predictions = (clf.fit(X_train, y_train)).predict(X_train)\n",
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"F1 score on training data: {:.4f}\".format(f1_score(y_train, unop_train_predictions)))\n",
    "print(\"F1-score on training data: \\n{}\".format(classification_report(y_train, unop_train_predictions)))\n",
    "\n",
    "# Optimized model on training data\n",
    "op_train_predictions = best_clf.predict(X_train)\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(\"Final F1 score on the training data: {:.4f}\".format(f1_score(y_train, op_train_predictions)))\n",
    "print(\"Final F1-score on the training data: \\n{}\".format(classification_report(y_train, op_train_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from seaborn import heatmap\n",
    "\n",
    "heatmap(confusion_matrix(y_valid, best_predictions), annot=True, fmt='d');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = best_clf.feature_importances_\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Rearrange feature names so they match the sorted feature importances\n",
    "names = [X_train.columns[i] for i in indices]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices])\n",
    "plt.xticks(range(X_train.shape[1]), names, rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "import pydotplus\n",
    "\n",
    "# from IPython.display import Image\n",
    "dot_data = export_graphviz(best_clf, out_file=None, feature_names=X_train.columns, class_names=['Not_Killed', 'Killed'], filled=True, rounded=True, special_characters=True, max_depth=5)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "\n",
    "graph.write_png('graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_vc = y_train[0].value_counts()\n",
    "class_weight = {0: train_vc[1] / train_vc[0], 1: 1}\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=20, min_samples_split=3, min_samples_leaf=4, class_weight=class_weight)\n",
    "\n",
    "clf.fit(X_train[0], y_train[0])\n",
    "y_pred_train = clf.predict(X_train[0])\n",
    "y_pred = clf.predict(X_valid[0])\n",
    "print(accuracy_score(y_train[0], y_pred_train))\n",
    "print(accuracy_score(y_valid[0], y_pred))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_train[0], y_pred_train, labels=[0, 1]))\n",
    "print(confusion_matrix(y_valid[0], y_pred, labels=[0, 1]))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_valid[0], y_pred, labels=[0, 1]))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "names = [X_train[0].columns[i] for i in indices]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.bar(range(X_train[0].shape[1]), importances[indices])\n",
    "plt.xticks(range(X_train[0].shape[1]), names, rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "import pydotplus\n",
    "\n",
    "# from IPython.display import Image\n",
    "dot_data = export_graphviz(clf, out_file=None, feature_names=X_train.columns, class_names=['Not_Killed', 'Killed'], filled=True, rounded=True, special_characters=True, max_depth=5)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "\n",
    "graph.write_png('graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=20, min_samples_split=3, min_samples_leaf=4, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred = clf.predict(X_valid)\n",
    "print(accuracy_score(y_train, y_pred_train))\n",
    "print(accuracy_score(y_valid, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_train, y_pred_train, labels=[0, 1]))\n",
    "print(confusion_matrix(y_valid, y_pred, labels=[0, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = clf.feature_importances_\n",
    "b = X_train.columns\n",
    "c = pd.DataFrame(a, index=b)\n",
    "c.sort_values(by=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_valid, y_pred, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "N_HIDDEN = 3\n",
    "N_BATCH = 10\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(DEVICE)\n",
    "X_valid_tensor = torch.tensor(X_valid.values, dtype=torch.float32).to(DEVICE)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(DEVICE)\n",
    "y_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size = 2 ** N_HIDDEN\n",
    "output_size = 1\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model_params = [\n",
    "    nn.Linear(input_size, hidden_size),\n",
    "]\n",
    "\n",
    "last_hidden_size = hidden_size\n",
    "for i in range(1, N_HIDDEN):\n",
    "    this_hidden_size = hidden_size // (2 ** i)\n",
    "    model_params += [\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(last_hidden_size, this_hidden_size),\n",
    "    ]\n",
    "    last_hidden_size = this_hidden_size\n",
    "model_params += [\n",
    "    nn.Softmax(dim=output_size),\n",
    "]\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(model_params)\n",
    "\n",
    "model = nn.Sequential(*model_params).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "EPOCHS = 50000\n",
    "losses = []\n",
    "epochs = []\n",
    "accs = []\n",
    "accs_valid = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    try:\n",
    "        optimizer.zero_grad()\n",
    "        X_perm = torch.randperm(X_train_tensor.size()[0])\n",
    "        X_batches = torch.split(X_train_tensor[X_perm][X_train_tensor.shape[0] % N_BATCH:], int((y_train.shape[0] / N_BATCH)))\n",
    "        y_batches = torch.split(y_train_tensor[X_perm][y_train_tensor.shape[0] % N_BATCH:], int((y_train.shape[0] / N_BATCH)))\n",
    "\n",
    "        batch_losses = []\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            acc_batch = []\n",
    "        else:\n",
    "            acc_batch = None\n",
    "\n",
    "        for i in range(len(X_batches)):\n",
    "            \n",
    "            y_pred = model(X_batches[i])[:, 1]\n",
    "            if acc_batch is not None:\n",
    "                acc_batch.append(f1_score(y_batches[i].detach().cpu().numpy(), np.where(y_pred.detach().cpu() > 0.5, 1, 0)))\n",
    "            loss = loss_function(y_pred, y_batches[i])\n",
    "            batch_losses.append(loss.item())\n",
    "            loss.backward()\n",
    "        losses.append(np.mean(batch_losses))\n",
    "        if epoch % 100 == 0:\n",
    "            acc = np.mean(acc_batch)\n",
    "            accs.append(acc)\n",
    "            y_pred_valid = model(X_valid_tensor)\n",
    "            y_pred_valid = y_pred_valid.squeeze(1)[:, 1].unsqueeze(1)\n",
    "            acc_valid = f1_score(y_valid_tensor.detach().cpu().numpy(), np.where(y_pred_valid.detach().cpu() > 0.5, 1, 0))\n",
    "            accs_valid.append(acc_valid)\n",
    "            print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {np.mean(losses[-100:]):.4f}, F1: {acc:.4f}, valid F1: {acc_valid:.4f}')\n",
    "        optimizer.step()\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Convert the valid data to PyTorch tensors and get the predicted class\n",
    "X_valid_tensor = torch.tensor(X_valid.values, dtype=torch.float32).to(DEVICE)\n",
    "y_pred_tensor = model(X_valid_tensor)[:, 1].unsqueeze(1).detach().cpu()\n",
    "\n",
    "y_pred = np.where(y_pred_tensor > 0.5, 1, 0)\n",
    "\n",
    "# _, y_pred = torch.max(y_pred_tensor, 1)\n",
    "\n",
    "# # Print the confusion matrix\n",
    "print(confusion_matrix(y_valid, y_pred))\n",
    "\n",
    "# # Print the accuracy\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_valid, y_pred))\n",
    "\n",
    "# plot the loss over the entire training procedure\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(accs, label='train')\n",
    "plt.plot(accs_valid, label='valid')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Linear(in_features=25, out_features=64, bias=True),\n",
      " Sigmoid(),\n",
      " Linear(in_features=64, out_features=16, bias=True),\n",
      " Sigmoid(),\n",
      " Linear(in_features=16, out_features=4, bias=True),\n",
      " Softmax(dim=1)]\n",
      "Epoch 1/50000, Loss: 0.3058, F1: 0.0000, valid F1: 0.0000\n",
      "Epoch 101/50000, Loss: 0.2498, F1: 0.4105, valid F1: 0.4161\n",
      "Epoch 201/50000, Loss: 0.2495, F1: 0.5852, valid F1: 0.5855\n",
      "Epoch 301/50000, Loss: 0.2489, F1: 0.5865, valid F1: 0.5838\n",
      "Epoch 401/50000, Loss: 0.2479, F1: 0.5848, valid F1: 0.5818\n",
      "Epoch 501/50000, Loss: 0.2459, F1: 0.5796, valid F1: 0.5813\n",
      "Epoch 601/50000, Loss: 0.2420, F1: 0.5840, valid F1: 0.5836\n",
      "Epoch 701/50000, Loss: 0.2373, F1: 0.5951, valid F1: 0.5928\n",
      "Epoch 801/50000, Loss: 0.2328, F1: 0.6056, valid F1: 0.6024\n",
      "Epoch 901/50000, Loss: 0.2255, F1: 0.6349, valid F1: 0.6305\n",
      "Epoch 1001/50000, Loss: 0.2141, F1: 0.6861, valid F1: 0.6816\n",
      "Epoch 1101/50000, Loss: 0.2017, F1: 0.7242, valid F1: 0.7202\n",
      "Epoch 1201/50000, Loss: 0.1909, F1: 0.7413, valid F1: 0.7383\n",
      "Epoch 1301/50000, Loss: 0.1811, F1: 0.7557, valid F1: 0.7526\n",
      "Epoch 1401/50000, Loss: 0.1723, F1: 0.7695, valid F1: 0.7705\n",
      "Epoch 1501/50000, Loss: 0.1653, F1: 0.7808, valid F1: 0.7819\n",
      "Epoch 1601/50000, Loss: 0.1602, F1: 0.7876, valid F1: 0.7863\n",
      "Epoch 1701/50000, Loss: 0.1566, F1: 0.7906, valid F1: 0.7896\n",
      "Epoch 1801/50000, Loss: 0.1542, F1: 0.7932, valid F1: 0.7924\n",
      "Epoch 1901/50000, Loss: 0.1526, F1: 0.7942, valid F1: 0.7930\n",
      "Epoch 2001/50000, Loss: 0.1514, F1: 0.7961, valid F1: 0.7960\n",
      "Epoch 2101/50000, Loss: 0.1506, F1: 0.7962, valid F1: 0.7954\n",
      "Epoch 2201/50000, Loss: 0.1501, F1: 0.7970, valid F1: 0.7964\n",
      "Epoch 2301/50000, Loss: 0.1497, F1: 0.7963, valid F1: 0.7960\n",
      "Epoch 2401/50000, Loss: 0.1493, F1: 0.7965, valid F1: 0.7955\n",
      "Epoch 2501/50000, Loss: 0.1491, F1: 0.7966, valid F1: 0.7957\n",
      "Epoch 2601/50000, Loss: 0.1489, F1: 0.7966, valid F1: 0.7956\n",
      "Epoch 2701/50000, Loss: 0.1487, F1: 0.7969, valid F1: 0.7957\n",
      "Epoch 2801/50000, Loss: 0.1486, F1: 0.7967, valid F1: 0.7955\n",
      "Epoch 2901/50000, Loss: 0.1484, F1: 0.7968, valid F1: 0.7957\n",
      "Epoch 3001/50000, Loss: 0.1483, F1: 0.7968, valid F1: 0.7957\n",
      "Epoch 3101/50000, Loss: 0.1482, F1: 0.7967, valid F1: 0.7958\n",
      "Epoch 3201/50000, Loss: 0.1480, F1: 0.7966, valid F1: 0.7957\n",
      "Epoch 3301/50000, Loss: 0.1479, F1: 0.7967, valid F1: 0.7959\n",
      "Epoch 3401/50000, Loss: 0.1477, F1: 0.7968, valid F1: 0.7956\n",
      "Epoch 3501/50000, Loss: 0.1476, F1: 0.7966, valid F1: 0.7957\n",
      "Epoch 3601/50000, Loss: 0.1474, F1: 0.7966, valid F1: 0.7958\n",
      "Epoch 3701/50000, Loss: 0.1473, F1: 0.7953, valid F1: 0.7945\n",
      "Epoch 3801/50000, Loss: 0.1470, F1: 0.7965, valid F1: 0.7960\n",
      "Epoch 3901/50000, Loss: 0.1468, F1: 0.7965, valid F1: 0.7963\n",
      "Epoch 4001/50000, Loss: 0.1463, F1: 0.7971, valid F1: 0.7968\n",
      "Epoch 4101/50000, Loss: 0.1459, F1: 0.7974, valid F1: 0.7982\n",
      "Epoch 4201/50000, Loss: 0.1453, F1: 0.7977, valid F1: 0.7985\n",
      "Epoch 4301/50000, Loss: 0.1448, F1: 0.7982, valid F1: 0.7993\n",
      "Epoch 4401/50000, Loss: 0.1442, F1: 0.7991, valid F1: 0.7992\n",
      "Epoch 4501/50000, Loss: 0.1435, F1: 0.8010, valid F1: 0.8000\n",
      "Epoch 4601/50000, Loss: 0.1426, F1: 0.8010, valid F1: 0.8001\n",
      "Epoch 4701/50000, Loss: 0.1417, F1: 0.8025, valid F1: 0.8004\n",
      "Epoch 4801/50000, Loss: 0.1407, F1: 0.8037, valid F1: 0.8018\n",
      "Epoch 4901/50000, Loss: 0.1395, F1: 0.8045, valid F1: 0.8031\n",
      "Epoch 5001/50000, Loss: 0.1383, F1: 0.8058, valid F1: 0.8046\n",
      "Epoch 5101/50000, Loss: 0.1371, F1: 0.8074, valid F1: 0.8046\n",
      "Epoch 5201/50000, Loss: 0.1358, F1: 0.8094, valid F1: 0.8061\n",
      "Epoch 5301/50000, Loss: 0.1345, F1: 0.8113, valid F1: 0.8074\n",
      "Epoch 5401/50000, Loss: 0.1332, F1: 0.8128, valid F1: 0.8097\n",
      "Epoch 5501/50000, Loss: 0.1317, F1: 0.8149, valid F1: 0.8116\n",
      "Epoch 5601/50000, Loss: 0.1303, F1: 0.8172, valid F1: 0.8133\n",
      "Epoch 5701/50000, Loss: 0.1288, F1: 0.8186, valid F1: 0.8153\n",
      "Epoch 5801/50000, Loss: 0.1274, F1: 0.8201, valid F1: 0.8165\n",
      "Epoch 5901/50000, Loss: 0.1260, F1: 0.8213, valid F1: 0.8178\n",
      "Epoch 6001/50000, Loss: 0.1245, F1: 0.8225, valid F1: 0.8184\n",
      "Epoch 6101/50000, Loss: 0.1230, F1: 0.8242, valid F1: 0.8193\n",
      "Epoch 6201/50000, Loss: 0.1215, F1: 0.8252, valid F1: 0.8205\n",
      "Epoch 6301/50000, Loss: 0.1200, F1: 0.8237, valid F1: 0.8201\n",
      "Epoch 6401/50000, Loss: 0.1187, F1: 0.8269, valid F1: 0.8227\n",
      "Epoch 6501/50000, Loss: 0.1175, F1: 0.8272, valid F1: 0.8233\n",
      "Epoch 6601/50000, Loss: 0.1165, F1: 0.8270, valid F1: 0.8239\n",
      "Epoch 6701/50000, Loss: 0.1156, F1: 0.8284, valid F1: 0.8252\n",
      "Epoch 6801/50000, Loss: 0.1148, F1: 0.8289, valid F1: 0.8259\n",
      "Epoch 6901/50000, Loss: 0.1144, F1: 0.8330, valid F1: 0.8297\n",
      "Epoch 7001/50000, Loss: 0.1135, F1: 0.8298, valid F1: 0.8268\n",
      "Epoch 7101/50000, Loss: 0.1130, F1: 0.8305, valid F1: 0.8268\n",
      "Epoch 7201/50000, Loss: 0.1125, F1: 0.8307, valid F1: 0.8271\n",
      "Epoch 7301/50000, Loss: 0.1120, F1: 0.8314, valid F1: 0.8272\n",
      "Epoch 7401/50000, Loss: 0.1116, F1: 0.8320, valid F1: 0.8286\n",
      "Epoch 7501/50000, Loss: 0.1112, F1: 0.8316, valid F1: 0.8274\n",
      "Epoch 7601/50000, Loss: 0.1109, F1: 0.8332, valid F1: 0.8289\n",
      "Epoch 7701/50000, Loss: 0.1106, F1: 0.8335, valid F1: 0.8293\n",
      "Epoch 7801/50000, Loss: 0.1102, F1: 0.8342, valid F1: 0.8299\n",
      "Epoch 7901/50000, Loss: 0.1099, F1: 0.8347, valid F1: 0.8307\n",
      "Epoch 8001/50000, Loss: 0.1096, F1: 0.8353, valid F1: 0.8317\n",
      "Epoch 8101/50000, Loss: 0.1094, F1: 0.8356, valid F1: 0.8320\n",
      "Epoch 8201/50000, Loss: 0.1091, F1: 0.8359, valid F1: 0.8326\n",
      "Epoch 8301/50000, Loss: 0.1088, F1: 0.8366, valid F1: 0.8330\n",
      "Epoch 8401/50000, Loss: 0.1086, F1: 0.8371, valid F1: 0.8335\n",
      "Epoch 8501/50000, Loss: 0.1083, F1: 0.8373, valid F1: 0.8340\n",
      "Epoch 8601/50000, Loss: 0.1081, F1: 0.8383, valid F1: 0.8344\n",
      "Epoch 8701/50000, Loss: 0.1078, F1: 0.8385, valid F1: 0.8345\n",
      "Epoch 8801/50000, Loss: 0.1076, F1: 0.8395, valid F1: 0.8356\n",
      "Epoch 8901/50000, Loss: 0.1074, F1: 0.8404, valid F1: 0.8358\n",
      "Epoch 9001/50000, Loss: 0.1071, F1: 0.8405, valid F1: 0.8367\n",
      "Epoch 9101/50000, Loss: 0.1069, F1: 0.8408, valid F1: 0.8373\n",
      "Epoch 9201/50000, Loss: 0.1067, F1: 0.8413, valid F1: 0.8377\n",
      "Epoch 9301/50000, Loss: 0.1065, F1: 0.8418, valid F1: 0.8381\n",
      "Epoch 9401/50000, Loss: 0.1063, F1: 0.8420, valid F1: 0.8386\n",
      "Epoch 9501/50000, Loss: 0.1061, F1: 0.8422, valid F1: 0.8388\n",
      "Epoch 9601/50000, Loss: 0.1061, F1: 0.8392, valid F1: 0.8363\n",
      "Epoch 9701/50000, Loss: 0.1058, F1: 0.8428, valid F1: 0.8397\n",
      "Epoch 9801/50000, Loss: 0.1056, F1: 0.8431, valid F1: 0.8399\n",
      "Epoch 9901/50000, Loss: 0.1054, F1: 0.8434, valid F1: 0.8401\n",
      "Epoch 10001/50000, Loss: 0.1052, F1: 0.8436, valid F1: 0.8404\n",
      "Epoch 10101/50000, Loss: 0.1051, F1: 0.8445, valid F1: 0.8407\n",
      "Epoch 10201/50000, Loss: 0.1049, F1: 0.8440, valid F1: 0.8409\n",
      "Epoch 10301/50000, Loss: 0.1047, F1: 0.8442, valid F1: 0.8411\n",
      "Epoch 10401/50000, Loss: 0.1046, F1: 0.8446, valid F1: 0.8413\n",
      "Epoch 10501/50000, Loss: 0.1044, F1: 0.8448, valid F1: 0.8414\n",
      "Epoch 10601/50000, Loss: 0.1043, F1: 0.8450, valid F1: 0.8416\n",
      "Epoch 10701/50000, Loss: 0.1041, F1: 0.8452, valid F1: 0.8417\n",
      "Epoch 10801/50000, Loss: 0.1041, F1: 0.8420, valid F1: 0.8399\n",
      "Epoch 10901/50000, Loss: 0.1038, F1: 0.8456, valid F1: 0.8422\n",
      "Epoch 11001/50000, Loss: 0.1037, F1: 0.8459, valid F1: 0.8425\n",
      "Epoch 11101/50000, Loss: 0.1036, F1: 0.8462, valid F1: 0.8424\n",
      "Epoch 11201/50000, Loss: 0.1034, F1: 0.8463, valid F1: 0.8424\n",
      "Epoch 11301/50000, Loss: 0.1033, F1: 0.8463, valid F1: 0.8428\n",
      "Epoch 11401/50000, Loss: 0.1032, F1: 0.8465, valid F1: 0.8430\n",
      "Epoch 11501/50000, Loss: 0.1031, F1: 0.8467, valid F1: 0.8431\n",
      "Epoch 11601/50000, Loss: 0.1030, F1: 0.8473, valid F1: 0.8437\n",
      "Epoch 11701/50000, Loss: 0.1029, F1: 0.8469, valid F1: 0.8432\n",
      "Epoch 11801/50000, Loss: 0.1028, F1: 0.8471, valid F1: 0.8439\n",
      "Epoch 11901/50000, Loss: 0.1027, F1: 0.8473, valid F1: 0.8442\n",
      "Epoch 12001/50000, Loss: 0.1026, F1: 0.8473, valid F1: 0.8444\n",
      "Epoch 12101/50000, Loss: 0.1025, F1: 0.8474, valid F1: 0.8443\n",
      "Epoch 12201/50000, Loss: 0.1024, F1: 0.8476, valid F1: 0.8443\n",
      "Epoch 12301/50000, Loss: 0.1026, F1: 0.8432, valid F1: 0.8402\n",
      "Epoch 12401/50000, Loss: 0.1022, F1: 0.8479, valid F1: 0.8445\n",
      "Epoch 12501/50000, Loss: 0.1021, F1: 0.8481, valid F1: 0.8447\n",
      "Epoch 12601/50000, Loss: 0.1021, F1: 0.8482, valid F1: 0.8447\n",
      "Epoch 12701/50000, Loss: 0.1020, F1: 0.8484, valid F1: 0.8448\n",
      "Epoch 12801/50000, Loss: 0.1019, F1: 0.8485, valid F1: 0.8445\n",
      "Epoch 12901/50000, Loss: 0.1018, F1: 0.8487, valid F1: 0.8448\n",
      "Epoch 13001/50000, Loss: 0.1018, F1: 0.8488, valid F1: 0.8449\n",
      "Epoch 13101/50000, Loss: 0.1017, F1: 0.8488, valid F1: 0.8448\n",
      "Epoch 13201/50000, Loss: 0.1017, F1: 0.8482, valid F1: 0.8438\n",
      "Epoch 13301/50000, Loss: 0.1016, F1: 0.8491, valid F1: 0.8455\n",
      "Epoch 13401/50000, Loss: 0.1015, F1: 0.8502, valid F1: 0.8477\n",
      "Epoch 13501/50000, Loss: 0.1014, F1: 0.8494, valid F1: 0.8459\n",
      "Epoch 13601/50000, Loss: 0.1014, F1: 0.8490, valid F1: 0.8451\n",
      "Epoch 13701/50000, Loss: 0.1013, F1: 0.8497, valid F1: 0.8462\n",
      "Epoch 13801/50000, Loss: 0.1013, F1: 0.8511, valid F1: 0.8484\n",
      "Epoch 13901/50000, Loss: 0.1012, F1: 0.8501, valid F1: 0.8466\n",
      "Epoch 14001/50000, Loss: 0.1011, F1: 0.8495, valid F1: 0.8458\n",
      "Epoch 14101/50000, Loss: 0.1011, F1: 0.8503, valid F1: 0.8470\n",
      "Epoch 14201/50000, Loss: 0.1010, F1: 0.8507, valid F1: 0.8475\n",
      "Epoch 14301/50000, Loss: 0.1010, F1: 0.8506, valid F1: 0.8474\n",
      "Epoch 14401/50000, Loss: 0.1009, F1: 0.8507, valid F1: 0.8478\n",
      "Epoch 14501/50000, Loss: 0.1008, F1: 0.8508, valid F1: 0.8472\n",
      "Epoch 14601/50000, Loss: 0.1008, F1: 0.8509, valid F1: 0.8475\n",
      "Epoch 14701/50000, Loss: 0.1007, F1: 0.8510, valid F1: 0.8477\n",
      "Epoch 14801/50000, Loss: 0.1007, F1: 0.8511, valid F1: 0.8479\n",
      "Epoch 14901/50000, Loss: 0.1006, F1: 0.8513, valid F1: 0.8480\n",
      "Epoch 15001/50000, Loss: 0.1006, F1: 0.8512, valid F1: 0.8480\n",
      "Epoch 15101/50000, Loss: 0.1005, F1: 0.8513, valid F1: 0.8481\n",
      "Epoch 15201/50000, Loss: 0.1005, F1: 0.8498, valid F1: 0.8458\n",
      "Epoch 15301/50000, Loss: 0.1004, F1: 0.8514, valid F1: 0.8481\n",
      "Epoch 15401/50000, Loss: 0.1004, F1: 0.8515, valid F1: 0.8480\n",
      "Epoch 15501/50000, Loss: 0.1004, F1: 0.8531, valid F1: 0.8505\n",
      "Epoch 15601/50000, Loss: 0.1003, F1: 0.8518, valid F1: 0.8487\n",
      "Epoch 15701/50000, Loss: 0.1002, F1: 0.8519, valid F1: 0.8488\n",
      "Epoch 15801/50000, Loss: 0.1002, F1: 0.8520, valid F1: 0.8487\n",
      "Epoch 15901/50000, Loss: 0.1001, F1: 0.8520, valid F1: 0.8487\n",
      "Epoch 16001/50000, Loss: 0.1001, F1: 0.8521, valid F1: 0.8488\n",
      "Epoch 16101/50000, Loss: 0.1000, F1: 0.8523, valid F1: 0.8490\n",
      "Epoch 16201/50000, Loss: 0.1000, F1: 0.8523, valid F1: 0.8493\n",
      "Epoch 16301/50000, Loss: 0.0999, F1: 0.8527, valid F1: 0.8494\n",
      "Epoch 16401/50000, Loss: 0.0999, F1: 0.8527, valid F1: 0.8494\n",
      "Epoch 16501/50000, Loss: 0.0999, F1: 0.8511, valid F1: 0.8477\n",
      "Epoch 16601/50000, Loss: 0.0998, F1: 0.8528, valid F1: 0.8495\n",
      "Epoch 16701/50000, Loss: 0.0998, F1: 0.8528, valid F1: 0.8495\n",
      "Epoch 16801/50000, Loss: 0.0997, F1: 0.8527, valid F1: 0.8495\n",
      "Epoch 16901/50000, Loss: 0.0997, F1: 0.8527, valid F1: 0.8495\n",
      "Epoch 17001/50000, Loss: 0.0998, F1: 0.8500, valid F1: 0.8460\n",
      "Epoch 17101/50000, Loss: 0.0996, F1: 0.8530, valid F1: 0.8494\n",
      "Epoch 17201/50000, Loss: 0.0996, F1: 0.8533, valid F1: 0.8499\n",
      "Epoch 17301/50000, Loss: 0.0995, F1: 0.8529, valid F1: 0.8494\n",
      "Epoch 17401/50000, Loss: 0.0995, F1: 0.8528, valid F1: 0.8491\n",
      "Epoch 17501/50000, Loss: 0.0995, F1: 0.8540, valid F1: 0.8503\n",
      "Epoch 17601/50000, Loss: 0.0994, F1: 0.8530, valid F1: 0.8495\n",
      "Epoch 17701/50000, Loss: 0.0994, F1: 0.8545, valid F1: 0.8506\n",
      "Epoch 17801/50000, Loss: 0.0993, F1: 0.8531, valid F1: 0.8496\n",
      "Epoch 17901/50000, Loss: 0.0993, F1: 0.8532, valid F1: 0.8495\n",
      "Epoch 18001/50000, Loss: 0.0993, F1: 0.8531, valid F1: 0.8496\n",
      "Epoch 18101/50000, Loss: 0.0994, F1: 0.8503, valid F1: 0.8466\n",
      "Epoch 18201/50000, Loss: 0.0992, F1: 0.8534, valid F1: 0.8495\n",
      "Epoch 18301/50000, Loss: 0.0992, F1: 0.8535, valid F1: 0.8495\n",
      "Epoch 18401/50000, Loss: 0.0991, F1: 0.8534, valid F1: 0.8493\n",
      "Epoch 18501/50000, Loss: 0.0991, F1: 0.8537, valid F1: 0.8496\n",
      "Epoch 18601/50000, Loss: 0.0990, F1: 0.8536, valid F1: 0.8496\n",
      "Epoch 18701/50000, Loss: 0.0990, F1: 0.8537, valid F1: 0.8497\n",
      "Epoch 18801/50000, Loss: 0.0990, F1: 0.8544, valid F1: 0.8503\n",
      "Epoch 18901/50000, Loss: 0.0989, F1: 0.8539, valid F1: 0.8499\n",
      "Epoch 19001/50000, Loss: 0.0989, F1: 0.8529, valid F1: 0.8487\n",
      "Epoch 19101/50000, Loss: 0.0989, F1: 0.8540, valid F1: 0.8499\n",
      "Epoch 19201/50000, Loss: 0.0988, F1: 0.8544, valid F1: 0.8503\n",
      "Epoch 19301/50000, Loss: 0.0988, F1: 0.8542, valid F1: 0.8501\n",
      "Epoch 19401/50000, Loss: 0.0988, F1: 0.8541, valid F1: 0.8498\n",
      "Epoch 19501/50000, Loss: 0.0987, F1: 0.8543, valid F1: 0.8499\n",
      "Epoch 19601/50000, Loss: 0.0987, F1: 0.8542, valid F1: 0.8499\n",
      "Epoch 19701/50000, Loss: 0.0987, F1: 0.8551, valid F1: 0.8511\n",
      "Epoch 19801/50000, Loss: 0.0986, F1: 0.8544, valid F1: 0.8501\n",
      "Epoch 19901/50000, Loss: 0.0988, F1: 0.8506, valid F1: 0.8469\n",
      "Epoch 20001/50000, Loss: 0.0986, F1: 0.8543, valid F1: 0.8501\n",
      "Epoch 20101/50000, Loss: 0.0987, F1: 0.8565, valid F1: 0.8530\n",
      "Epoch 20201/50000, Loss: 0.0985, F1: 0.8545, valid F1: 0.8503\n",
      "Epoch 20301/50000, Loss: 0.0985, F1: 0.8535, valid F1: 0.8488\n",
      "Epoch 20401/50000, Loss: 0.0984, F1: 0.8546, valid F1: 0.8503\n",
      "Epoch 20501/50000, Loss: 0.0984, F1: 0.8527, valid F1: 0.8482\n",
      "Epoch 20601/50000, Loss: 0.0983, F1: 0.8549, valid F1: 0.8502\n",
      "Epoch 20701/50000, Loss: 0.0983, F1: 0.8552, valid F1: 0.8510\n",
      "Epoch 20801/50000, Loss: 0.0983, F1: 0.8550, valid F1: 0.8501\n",
      "Epoch 20901/50000, Loss: 0.0982, F1: 0.8544, valid F1: 0.8493\n",
      "Epoch 21001/50000, Loss: 0.0982, F1: 0.8551, valid F1: 0.8499\n",
      "Epoch 21101/50000, Loss: 0.0982, F1: 0.8549, valid F1: 0.8499\n",
      "Epoch 21201/50000, Loss: 0.0981, F1: 0.8551, valid F1: 0.8500\n",
      "Epoch 21301/50000, Loss: 0.0981, F1: 0.8553, valid F1: 0.8500\n",
      "Epoch 21401/50000, Loss: 0.0981, F1: 0.8551, valid F1: 0.8500\n",
      "Epoch 21501/50000, Loss: 0.0980, F1: 0.8559, valid F1: 0.8511\n",
      "Epoch 21601/50000, Loss: 0.0980, F1: 0.8553, valid F1: 0.8502\n",
      "Epoch 21701/50000, Loss: 0.0980, F1: 0.8535, valid F1: 0.8482\n",
      "Epoch 21801/50000, Loss: 0.0979, F1: 0.8556, valid F1: 0.8503\n",
      "Epoch 21901/50000, Loss: 0.0979, F1: 0.8536, valid F1: 0.8483\n",
      "Epoch 22001/50000, Loss: 0.0978, F1: 0.8559, valid F1: 0.8505\n",
      "Epoch 22101/50000, Loss: 0.0978, F1: 0.8556, valid F1: 0.8502\n",
      "Epoch 22201/50000, Loss: 0.0978, F1: 0.8561, valid F1: 0.8507\n",
      "Epoch 22301/50000, Loss: 0.0977, F1: 0.8560, valid F1: 0.8504\n",
      "Epoch 22401/50000, Loss: 0.0978, F1: 0.8580, valid F1: 0.8533\n",
      "Epoch 22501/50000, Loss: 0.0976, F1: 0.8563, valid F1: 0.8510\n",
      "Epoch 22601/50000, Loss: 0.0976, F1: 0.8565, valid F1: 0.8516\n",
      "Epoch 22701/50000, Loss: 0.0976, F1: 0.8545, valid F1: 0.8491\n",
      "Epoch 22801/50000, Loss: 0.0975, F1: 0.8563, valid F1: 0.8516\n",
      "Epoch 22901/50000, Loss: 0.0975, F1: 0.8564, valid F1: 0.8516\n",
      "Epoch 23001/50000, Loss: 0.0975, F1: 0.8548, valid F1: 0.8492\n",
      "Epoch 23101/50000, Loss: 0.0974, F1: 0.8566, valid F1: 0.8519\n",
      "Epoch 23201/50000, Loss: 0.0974, F1: 0.8570, valid F1: 0.8522\n",
      "Epoch 23301/50000, Loss: 0.0973, F1: 0.8563, valid F1: 0.8514\n",
      "Epoch 23401/50000, Loss: 0.0973, F1: 0.8570, valid F1: 0.8523\n",
      "Epoch 23501/50000, Loss: 0.0973, F1: 0.8581, valid F1: 0.8532\n",
      "Epoch 23601/50000, Loss: 0.0972, F1: 0.8572, valid F1: 0.8525\n",
      "Epoch 23701/50000, Loss: 0.0972, F1: 0.8582, valid F1: 0.8534\n",
      "Epoch 23801/50000, Loss: 0.0971, F1: 0.8574, valid F1: 0.8526\n",
      "Epoch 23901/50000, Loss: 0.0971, F1: 0.8571, valid F1: 0.8524\n",
      "Epoch 24001/50000, Loss: 0.0970, F1: 0.8573, valid F1: 0.8527\n",
      "Epoch 24101/50000, Loss: 0.0970, F1: 0.8578, valid F1: 0.8526\n",
      "Epoch 24201/50000, Loss: 0.0969, F1: 0.8575, valid F1: 0.8527\n",
      "Epoch 24301/50000, Loss: 0.0969, F1: 0.8570, valid F1: 0.8527\n",
      "Epoch 24401/50000, Loss: 0.0968, F1: 0.8574, valid F1: 0.8529\n",
      "Epoch 24501/50000, Loss: 0.0968, F1: 0.8565, valid F1: 0.8520\n",
      "Epoch 24601/50000, Loss: 0.0968, F1: 0.8577, valid F1: 0.8532\n",
      "Epoch 24701/50000, Loss: 0.0967, F1: 0.8577, valid F1: 0.8533\n",
      "Epoch 24801/50000, Loss: 0.0967, F1: 0.8578, valid F1: 0.8532\n",
      "Epoch 24901/50000, Loss: 0.0966, F1: 0.8579, valid F1: 0.8532\n",
      "Epoch 25001/50000, Loss: 0.0966, F1: 0.8581, valid F1: 0.8536\n",
      "Epoch 25101/50000, Loss: 0.0965, F1: 0.8581, valid F1: 0.8534\n",
      "Epoch 25201/50000, Loss: 0.0965, F1: 0.8582, valid F1: 0.8536\n",
      "Epoch 25301/50000, Loss: 0.0964, F1: 0.8581, valid F1: 0.8535\n",
      "Epoch 25401/50000, Loss: 0.0964, F1: 0.8582, valid F1: 0.8532\n",
      "Epoch 25501/50000, Loss: 0.0964, F1: 0.8599, valid F1: 0.8550\n",
      "Epoch 25601/50000, Loss: 0.0963, F1: 0.8583, valid F1: 0.8536\n",
      "Epoch 25701/50000, Loss: 0.0963, F1: 0.8590, valid F1: 0.8542\n",
      "Epoch 25801/50000, Loss: 0.0962, F1: 0.8596, valid F1: 0.8543\n",
      "Epoch 25901/50000, Loss: 0.0962, F1: 0.8588, valid F1: 0.8540\n",
      "Epoch 26001/50000, Loss: 0.0961, F1: 0.8588, valid F1: 0.8539\n",
      "Epoch 26101/50000, Loss: 0.0961, F1: 0.8596, valid F1: 0.8549\n",
      "Epoch 26201/50000, Loss: 0.0961, F1: 0.8599, valid F1: 0.8556\n",
      "Epoch 26301/50000, Loss: 0.0960, F1: 0.8593, valid F1: 0.8540\n",
      "Epoch 26401/50000, Loss: 0.0959, F1: 0.8593, valid F1: 0.8540\n",
      "Epoch 26501/50000, Loss: 0.0959, F1: 0.8594, valid F1: 0.8550\n",
      "Epoch 26601/50000, Loss: 0.0958, F1: 0.8585, valid F1: 0.8532\n",
      "Epoch 26701/50000, Loss: 0.0959, F1: 0.8607, valid F1: 0.8560\n",
      "Epoch 26801/50000, Loss: 0.0957, F1: 0.8595, valid F1: 0.8550\n",
      "Epoch 26901/50000, Loss: 0.0957, F1: 0.8595, valid F1: 0.8551\n",
      "Epoch 27001/50000, Loss: 0.0956, F1: 0.8594, valid F1: 0.8553\n",
      "Epoch 27101/50000, Loss: 0.0956, F1: 0.8590, valid F1: 0.8542\n",
      "Epoch 27201/50000, Loss: 0.0957, F1: 0.8618, valid F1: 0.8567\n",
      "Epoch 27301/50000, Loss: 0.0955, F1: 0.8595, valid F1: 0.8551\n",
      "Epoch 27401/50000, Loss: 0.0954, F1: 0.8596, valid F1: 0.8551\n",
      "Epoch 27501/50000, Loss: 0.0954, F1: 0.8598, valid F1: 0.8556\n",
      "Epoch 27601/50000, Loss: 0.0953, F1: 0.8597, valid F1: 0.8554\n",
      "Epoch 27701/50000, Loss: 0.0953, F1: 0.8600, valid F1: 0.8554\n",
      "Epoch 27801/50000, Loss: 0.0952, F1: 0.8599, valid F1: 0.8555\n",
      "Epoch 27901/50000, Loss: 0.0952, F1: 0.8602, valid F1: 0.8556\n",
      "Epoch 28001/50000, Loss: 0.0951, F1: 0.8602, valid F1: 0.8557\n",
      "Epoch 28101/50000, Loss: 0.0951, F1: 0.8595, valid F1: 0.8553\n",
      "Epoch 28201/50000, Loss: 0.0951, F1: 0.8618, valid F1: 0.8571\n",
      "Epoch 28301/50000, Loss: 0.0950, F1: 0.8590, valid F1: 0.8546\n",
      "Epoch 28401/50000, Loss: 0.0951, F1: 0.8625, valid F1: 0.8577\n",
      "Epoch 28501/50000, Loss: 0.0949, F1: 0.8609, valid F1: 0.8560\n",
      "Epoch 28601/50000, Loss: 0.0948, F1: 0.8608, valid F1: 0.8558\n",
      "Epoch 28701/50000, Loss: 0.0948, F1: 0.8609, valid F1: 0.8559\n",
      "Epoch 28801/50000, Loss: 0.0949, F1: 0.8631, valid F1: 0.8583\n",
      "Epoch 28901/50000, Loss: 0.0947, F1: 0.8610, valid F1: 0.8561\n",
      "Epoch 29001/50000, Loss: 0.0946, F1: 0.8611, valid F1: 0.8565\n",
      "Epoch 29101/50000, Loss: 0.0946, F1: 0.8611, valid F1: 0.8565\n",
      "Epoch 29201/50000, Loss: 0.0946, F1: 0.8620, valid F1: 0.8578\n",
      "Epoch 29301/50000, Loss: 0.0947, F1: 0.8582, valid F1: 0.8549\n",
      "Epoch 29401/50000, Loss: 0.0944, F1: 0.8609, valid F1: 0.8565\n",
      "Epoch 29501/50000, Loss: 0.0944, F1: 0.8612, valid F1: 0.8572\n",
      "Epoch 29601/50000, Loss: 0.0943, F1: 0.8610, valid F1: 0.8569\n",
      "Epoch 29701/50000, Loss: 0.0943, F1: 0.8626, valid F1: 0.8578\n",
      "Epoch 29801/50000, Loss: 0.0943, F1: 0.8625, valid F1: 0.8580\n",
      "Epoch 29901/50000, Loss: 0.0942, F1: 0.8615, valid F1: 0.8576\n",
      "Epoch 30001/50000, Loss: 0.0941, F1: 0.8616, valid F1: 0.8577\n",
      "Epoch 30101/50000, Loss: 0.0941, F1: 0.8616, valid F1: 0.8572\n",
      "Epoch 30201/50000, Loss: 0.0941, F1: 0.8626, valid F1: 0.8581\n",
      "Epoch 30301/50000, Loss: 0.0940, F1: 0.8612, valid F1: 0.8568\n",
      "Epoch 30401/50000, Loss: 0.0940, F1: 0.8634, valid F1: 0.8585\n",
      "Epoch 30501/50000, Loss: 0.0939, F1: 0.8609, valid F1: 0.8565\n",
      "Epoch 30601/50000, Loss: 0.0939, F1: 0.8626, valid F1: 0.8583\n",
      "Epoch 30701/50000, Loss: 0.0938, F1: 0.8629, valid F1: 0.8585\n",
      "Epoch 30801/50000, Loss: 0.0938, F1: 0.8618, valid F1: 0.8574\n",
      "Epoch 30901/50000, Loss: 0.0939, F1: 0.8648, valid F1: 0.8609\n",
      "Epoch 31001/50000, Loss: 0.0937, F1: 0.8624, valid F1: 0.8583\n",
      "Epoch 31101/50000, Loss: 0.0936, F1: 0.8626, valid F1: 0.8586\n",
      "Epoch 31201/50000, Loss: 0.0936, F1: 0.8628, valid F1: 0.8586\n",
      "Epoch 31301/50000, Loss: 0.0935, F1: 0.8622, valid F1: 0.8578\n",
      "Epoch 31401/50000, Loss: 0.0935, F1: 0.8628, valid F1: 0.8586\n",
      "Epoch 31501/50000, Loss: 0.0934, F1: 0.8629, valid F1: 0.8587\n",
      "Epoch 31601/50000, Loss: 0.0936, F1: 0.8657, valid F1: 0.8614\n",
      "Epoch 31701/50000, Loss: 0.0934, F1: 0.8631, valid F1: 0.8589\n",
      "Epoch 31801/50000, Loss: 0.0935, F1: 0.8658, valid F1: 0.8617\n",
      "Epoch 31901/50000, Loss: 0.0933, F1: 0.8633, valid F1: 0.8589\n",
      "Epoch 32001/50000, Loss: 0.0933, F1: 0.8617, valid F1: 0.8578\n",
      "Epoch 32101/50000, Loss: 0.0932, F1: 0.8634, valid F1: 0.8587\n",
      "Epoch 32201/50000, Loss: 0.0931, F1: 0.8634, valid F1: 0.8589\n",
      "Epoch 32301/50000, Loss: 0.0931, F1: 0.8631, valid F1: 0.8588\n",
      "Epoch 32401/50000, Loss: 0.0930, F1: 0.8637, valid F1: 0.8592\n",
      "Epoch 32501/50000, Loss: 0.0931, F1: 0.8614, valid F1: 0.8577\n",
      "Epoch 32601/50000, Loss: 0.0930, F1: 0.8636, valid F1: 0.8595\n",
      "Epoch 32701/50000, Loss: 0.0929, F1: 0.8637, valid F1: 0.8595\n",
      "Epoch 32801/50000, Loss: 0.0930, F1: 0.8662, valid F1: 0.8621\n",
      "Epoch 32901/50000, Loss: 0.0928, F1: 0.8638, valid F1: 0.8596\n",
      "Epoch 33001/50000, Loss: 0.0928, F1: 0.8640, valid F1: 0.8599\n",
      "Epoch 33101/50000, Loss: 0.0928, F1: 0.8633, valid F1: 0.8596\n",
      "Epoch 33201/50000, Loss: 0.0927, F1: 0.8651, valid F1: 0.8604\n",
      "Epoch 33301/50000, Loss: 0.0927, F1: 0.8656, valid F1: 0.8612\n",
      "Epoch 33401/50000, Loss: 0.0926, F1: 0.8642, valid F1: 0.8602\n",
      "Epoch 33501/50000, Loss: 0.0926, F1: 0.8646, valid F1: 0.8603\n",
      "Epoch 33601/50000, Loss: 0.0926, F1: 0.8643, valid F1: 0.8604\n",
      "Epoch 33701/50000, Loss: 0.0925, F1: 0.8642, valid F1: 0.8604\n",
      "Epoch 33801/50000, Loss: 0.0925, F1: 0.8644, valid F1: 0.8606\n",
      "Epoch 33901/50000, Loss: 0.0924, F1: 0.8646, valid F1: 0.8604\n",
      "Epoch 34001/50000, Loss: 0.0924, F1: 0.8650, valid F1: 0.8608\n",
      "Epoch 34101/50000, Loss: 0.0924, F1: 0.8645, valid F1: 0.8605\n",
      "Epoch 34201/50000, Loss: 0.0923, F1: 0.8647, valid F1: 0.8606\n",
      "Epoch 34301/50000, Loss: 0.0923, F1: 0.8660, valid F1: 0.8618\n",
      "Epoch 34401/50000, Loss: 0.0923, F1: 0.8642, valid F1: 0.8608\n",
      "Epoch 34501/50000, Loss: 0.0922, F1: 0.8653, valid F1: 0.8610\n",
      "Epoch 34601/50000, Loss: 0.0922, F1: 0.8652, valid F1: 0.8609\n",
      "Epoch 34701/50000, Loss: 0.0922, F1: 0.8654, valid F1: 0.8613\n",
      "Epoch 34801/50000, Loss: 0.0921, F1: 0.8653, valid F1: 0.8612\n",
      "Epoch 34901/50000, Loss: 0.0921, F1: 0.8649, valid F1: 0.8609\n",
      "Epoch 35001/50000, Loss: 0.0921, F1: 0.8643, valid F1: 0.8606\n",
      "Epoch 35101/50000, Loss: 0.0920, F1: 0.8656, valid F1: 0.8619\n",
      "Epoch 35201/50000, Loss: 0.0920, F1: 0.8660, valid F1: 0.8619\n",
      "Epoch 35301/50000, Loss: 0.0920, F1: 0.8648, valid F1: 0.8611\n",
      "Epoch 35401/50000, Loss: 0.0920, F1: 0.8666, valid F1: 0.8625\n",
      "Epoch 35501/50000, Loss: 0.0919, F1: 0.8654, valid F1: 0.8614\n",
      "Epoch 35601/50000, Loss: 0.0919, F1: 0.8655, valid F1: 0.8620\n",
      "Epoch 35701/50000, Loss: 0.0919, F1: 0.8668, valid F1: 0.8628\n",
      "Epoch 35801/50000, Loss: 0.0918, F1: 0.8655, valid F1: 0.8619\n",
      "Epoch 35901/50000, Loss: 0.0918, F1: 0.8654, valid F1: 0.8618\n",
      "Epoch 36001/50000, Loss: 0.0917, F1: 0.8655, valid F1: 0.8620\n",
      "Epoch 36101/50000, Loss: 0.0917, F1: 0.8657, valid F1: 0.8623\n",
      "Epoch 36201/50000, Loss: 0.0917, F1: 0.8655, valid F1: 0.8621\n",
      "Epoch 36301/50000, Loss: 0.0916, F1: 0.8653, valid F1: 0.8621\n",
      "Epoch 36401/50000, Loss: 0.0916, F1: 0.8658, valid F1: 0.8625\n",
      "Epoch 36501/50000, Loss: 0.0916, F1: 0.8659, valid F1: 0.8624\n",
      "Epoch 36601/50000, Loss: 0.0916, F1: 0.8657, valid F1: 0.8625\n",
      "Epoch 36701/50000, Loss: 0.0916, F1: 0.8674, valid F1: 0.8639\n",
      "Epoch 36801/50000, Loss: 0.0915, F1: 0.8668, valid F1: 0.8632\n",
      "Epoch 36901/50000, Loss: 0.0915, F1: 0.8659, valid F1: 0.8626\n",
      "Epoch 37001/50000, Loss: 0.0914, F1: 0.8659, valid F1: 0.8628\n",
      "Epoch 37101/50000, Loss: 0.0915, F1: 0.8653, valid F1: 0.8618\n",
      "Epoch 37201/50000, Loss: 0.0914, F1: 0.8673, valid F1: 0.8639\n",
      "Epoch 37301/50000, Loss: 0.0913, F1: 0.8664, valid F1: 0.8633\n",
      "Epoch 37401/50000, Loss: 0.0913, F1: 0.8664, valid F1: 0.8630\n",
      "Epoch 37501/50000, Loss: 0.0914, F1: 0.8650, valid F1: 0.8615\n",
      "Epoch 37601/50000, Loss: 0.0913, F1: 0.8668, valid F1: 0.8636\n",
      "Epoch 37701/50000, Loss: 0.0912, F1: 0.8660, valid F1: 0.8626\n",
      "Epoch 37801/50000, Loss: 0.0912, F1: 0.8665, valid F1: 0.8632\n",
      "Epoch 37901/50000, Loss: 0.0912, F1: 0.8666, valid F1: 0.8634\n",
      "Epoch 38001/50000, Loss: 0.0912, F1: 0.8666, valid F1: 0.8636\n",
      "Epoch 38101/50000, Loss: 0.0911, F1: 0.8671, valid F1: 0.8640\n",
      "Epoch 38201/50000, Loss: 0.0911, F1: 0.8673, valid F1: 0.8638\n",
      "Epoch 38301/50000, Loss: 0.0911, F1: 0.8667, valid F1: 0.8637\n",
      "Epoch 38401/50000, Loss: 0.0911, F1: 0.8673, valid F1: 0.8637\n",
      "Epoch 38501/50000, Loss: 0.0910, F1: 0.8663, valid F1: 0.8630\n",
      "Epoch 38601/50000, Loss: 0.0910, F1: 0.8666, valid F1: 0.8631\n",
      "Epoch 38701/50000, Loss: 0.0910, F1: 0.8663, valid F1: 0.8623\n",
      "Epoch 38801/50000, Loss: 0.0910, F1: 0.8662, valid F1: 0.8623\n",
      "Epoch 38901/50000, Loss: 0.0909, F1: 0.8674, valid F1: 0.8640\n",
      "Epoch 39001/50000, Loss: 0.0909, F1: 0.8671, valid F1: 0.8641\n",
      "Epoch 39101/50000, Loss: 0.0909, F1: 0.8675, valid F1: 0.8640\n",
      "Epoch 39201/50000, Loss: 0.0908, F1: 0.8669, valid F1: 0.8631\n",
      "Epoch 39301/50000, Loss: 0.0908, F1: 0.8665, valid F1: 0.8626\n",
      "Epoch 39401/50000, Loss: 0.0908, F1: 0.8674, valid F1: 0.8642\n",
      "Epoch 39501/50000, Loss: 0.0908, F1: 0.8676, valid F1: 0.8641\n",
      "Epoch 39601/50000, Loss: 0.0907, F1: 0.8669, valid F1: 0.8631\n",
      "Epoch 39701/50000, Loss: 0.0907, F1: 0.8672, valid F1: 0.8636\n",
      "Epoch 39801/50000, Loss: 0.0907, F1: 0.8673, valid F1: 0.8636\n",
      "Epoch 39901/50000, Loss: 0.0906, F1: 0.8673, valid F1: 0.8637\n",
      "Epoch 40001/50000, Loss: 0.0906, F1: 0.8673, valid F1: 0.8636\n",
      "Epoch 40101/50000, Loss: 0.0907, F1: 0.8685, valid F1: 0.8650\n",
      "Epoch 40201/50000, Loss: 0.0906, F1: 0.8672, valid F1: 0.8635\n",
      "Epoch 40301/50000, Loss: 0.0906, F1: 0.8673, valid F1: 0.8636\n",
      "Epoch 40401/50000, Loss: 0.0906, F1: 0.8685, valid F1: 0.8649\n",
      "Epoch 40501/50000, Loss: 0.0906, F1: 0.8687, valid F1: 0.8648\n",
      "Epoch 40601/50000, Loss: 0.0905, F1: 0.8682, valid F1: 0.8645\n",
      "Epoch 40701/50000, Loss: 0.0905, F1: 0.8676, valid F1: 0.8639\n",
      "Epoch 40801/50000, Loss: 0.0904, F1: 0.8676, valid F1: 0.8641\n",
      "Epoch 40901/50000, Loss: 0.0904, F1: 0.8676, valid F1: 0.8643\n",
      "Epoch 41001/50000, Loss: 0.0904, F1: 0.8673, valid F1: 0.8636\n",
      "Epoch 41101/50000, Loss: 0.0904, F1: 0.8678, valid F1: 0.8644\n",
      "Epoch 41201/50000, Loss: 0.0903, F1: 0.8676, valid F1: 0.8641\n",
      "Epoch 41301/50000, Loss: 0.0903, F1: 0.8677, valid F1: 0.8641\n",
      "Epoch 41401/50000, Loss: 0.0903, F1: 0.8680, valid F1: 0.8648\n",
      "Epoch 41501/50000, Loss: 0.0903, F1: 0.8682, valid F1: 0.8649\n",
      "Epoch 41601/50000, Loss: 0.0902, F1: 0.8677, valid F1: 0.8643\n",
      "Epoch 41701/50000, Loss: 0.0903, F1: 0.8673, valid F1: 0.8638\n",
      "Epoch 41801/50000, Loss: 0.0902, F1: 0.8677, valid F1: 0.8644\n",
      "Epoch 41901/50000, Loss: 0.0902, F1: 0.8680, valid F1: 0.8649\n",
      "Epoch 42001/50000, Loss: 0.0902, F1: 0.8683, valid F1: 0.8652\n",
      "Epoch 42101/50000, Loss: 0.0902, F1: 0.8672, valid F1: 0.8642\n",
      "Epoch 42201/50000, Loss: 0.0901, F1: 0.8677, valid F1: 0.8646\n",
      "Epoch 42301/50000, Loss: 0.0901, F1: 0.8674, valid F1: 0.8647\n",
      "Epoch 42401/50000, Loss: 0.0901, F1: 0.8678, valid F1: 0.8653\n",
      "Epoch 42501/50000, Loss: 0.0901, F1: 0.8678, valid F1: 0.8648\n",
      "Epoch 42601/50000, Loss: 0.0900, F1: 0.8681, valid F1: 0.8654\n",
      "Epoch 42701/50000, Loss: 0.0900, F1: 0.8686, valid F1: 0.8657\n",
      "Epoch 42801/50000, Loss: 0.0900, F1: 0.8674, valid F1: 0.8647\n",
      "Epoch 42901/50000, Loss: 0.0900, F1: 0.8689, valid F1: 0.8665\n",
      "Epoch 43001/50000, Loss: 0.0899, F1: 0.8683, valid F1: 0.8657\n",
      "Epoch 43101/50000, Loss: 0.0900, F1: 0.8678, valid F1: 0.8653\n",
      "Epoch 43201/50000, Loss: 0.0899, F1: 0.8690, valid F1: 0.8666\n",
      "Epoch 43301/50000, Loss: 0.0899, F1: 0.8697, valid F1: 0.8669\n",
      "Epoch 43401/50000, Loss: 0.0899, F1: 0.8690, valid F1: 0.8667\n",
      "Epoch 43501/50000, Loss: 0.0898, F1: 0.8683, valid F1: 0.8657\n",
      "Epoch 43601/50000, Loss: 0.0898, F1: 0.8681, valid F1: 0.8659\n",
      "Epoch 43701/50000, Loss: 0.0898, F1: 0.8687, valid F1: 0.8662\n",
      "Epoch 43801/50000, Loss: 0.0898, F1: 0.8690, valid F1: 0.8664\n",
      "Epoch 43901/50000, Loss: 0.0898, F1: 0.8691, valid F1: 0.8666\n",
      "Epoch 44001/50000, Loss: 0.0898, F1: 0.8696, valid F1: 0.8673\n",
      "Epoch 44101/50000, Loss: 0.0897, F1: 0.8696, valid F1: 0.8675\n",
      "Epoch 44201/50000, Loss: 0.0897, F1: 0.8687, valid F1: 0.8667\n",
      "Epoch 44301/50000, Loss: 0.0897, F1: 0.8692, valid F1: 0.8668\n",
      "Epoch 44401/50000, Loss: 0.0897, F1: 0.8686, valid F1: 0.8664\n",
      "Epoch 44501/50000, Loss: 0.0897, F1: 0.8695, valid F1: 0.8673\n",
      "Epoch 44601/50000, Loss: 0.0897, F1: 0.8705, valid F1: 0.8675\n",
      "Epoch 44701/50000, Loss: 0.0896, F1: 0.8693, valid F1: 0.8671\n",
      "Epoch 44801/50000, Loss: 0.0896, F1: 0.8695, valid F1: 0.8672\n",
      "Epoch 44901/50000, Loss: 0.0896, F1: 0.8695, valid F1: 0.8670\n",
      "Epoch 45001/50000, Loss: 0.0896, F1: 0.8698, valid F1: 0.8676\n",
      "Epoch 45101/50000, Loss: 0.0896, F1: 0.8694, valid F1: 0.8668\n",
      "Epoch 45201/50000, Loss: 0.0895, F1: 0.8695, valid F1: 0.8670\n",
      "Epoch 45301/50000, Loss: 0.0895, F1: 0.8697, valid F1: 0.8673\n",
      "Epoch 45401/50000, Loss: 0.0896, F1: 0.8687, valid F1: 0.8665\n",
      "Epoch 45501/50000, Loss: 0.0895, F1: 0.8700, valid F1: 0.8677\n",
      "Epoch 45601/50000, Loss: 0.0895, F1: 0.8689, valid F1: 0.8669\n",
      "Epoch 45701/50000, Loss: 0.0895, F1: 0.8698, valid F1: 0.8674\n",
      "Epoch 45801/50000, Loss: 0.0895, F1: 0.8697, valid F1: 0.8675\n",
      "Epoch 45901/50000, Loss: 0.0894, F1: 0.8700, valid F1: 0.8676\n",
      "Epoch 46001/50000, Loss: 0.0894, F1: 0.8694, valid F1: 0.8674\n",
      "Epoch 46101/50000, Loss: 0.0894, F1: 0.8688, valid F1: 0.8670\n",
      "Epoch 46201/50000, Loss: 0.0894, F1: 0.8700, valid F1: 0.8675\n",
      "Epoch 46301/50000, Loss: 0.0895, F1: 0.8715, valid F1: 0.8675\n",
      "Epoch 46401/50000, Loss: 0.0895, F1: 0.8682, valid F1: 0.8663\n",
      "Epoch 46501/50000, Loss: 0.0894, F1: 0.8693, valid F1: 0.8670\n",
      "Epoch 46601/50000, Loss: 0.0894, F1: 0.8708, valid F1: 0.8679\n",
      "Epoch 46701/50000, Loss: 0.0894, F1: 0.8707, valid F1: 0.8680\n",
      "Epoch 46801/50000, Loss: 0.0893, F1: 0.8703, valid F1: 0.8679\n",
      "Epoch 46901/50000, Loss: 0.0893, F1: 0.8701, valid F1: 0.8678\n",
      "Epoch 47001/50000, Loss: 0.0893, F1: 0.8698, valid F1: 0.8672\n",
      "Epoch 47101/50000, Loss: 0.0893, F1: 0.8697, valid F1: 0.8674\n",
      "Epoch 47201/50000, Loss: 0.0893, F1: 0.8701, valid F1: 0.8678\n",
      "Epoch 47301/50000, Loss: 0.0893, F1: 0.8706, valid F1: 0.8682\n",
      "Epoch 47401/50000, Loss: 0.0892, F1: 0.8698, valid F1: 0.8674\n",
      "Epoch 47501/50000, Loss: 0.0892, F1: 0.8699, valid F1: 0.8675\n",
      "Epoch 47601/50000, Loss: 0.0892, F1: 0.8704, valid F1: 0.8678\n",
      "Epoch 47701/50000, Loss: 0.0892, F1: 0.8706, valid F1: 0.8680\n",
      "Epoch 47801/50000, Loss: 0.0892, F1: 0.8696, valid F1: 0.8672\n",
      "Epoch 47901/50000, Loss: 0.0892, F1: 0.8701, valid F1: 0.8679\n",
      "Epoch 48001/50000, Loss: 0.0892, F1: 0.8697, valid F1: 0.8672\n",
      "Epoch 48101/50000, Loss: 0.0892, F1: 0.8707, valid F1: 0.8682\n",
      "Epoch 48201/50000, Loss: 0.0892, F1: 0.8716, valid F1: 0.8681\n",
      "Epoch 48301/50000, Loss: 0.0891, F1: 0.8706, valid F1: 0.8682\n",
      "Epoch 48401/50000, Loss: 0.0891, F1: 0.8707, valid F1: 0.8680\n",
      "Epoch 48501/50000, Loss: 0.0891, F1: 0.8701, valid F1: 0.8676\n",
      "Epoch 48601/50000, Loss: 0.0891, F1: 0.8705, valid F1: 0.8679\n",
      "Epoch 48701/50000, Loss: 0.0891, F1: 0.8700, valid F1: 0.8673\n",
      "Epoch 48801/50000, Loss: 0.0891, F1: 0.8709, valid F1: 0.8681\n",
      "Epoch 48901/50000, Loss: 0.0891, F1: 0.8714, valid F1: 0.8678\n",
      "Epoch 49001/50000, Loss: 0.0891, F1: 0.8699, valid F1: 0.8674\n",
      "Epoch 49101/50000, Loss: 0.0890, F1: 0.8708, valid F1: 0.8679\n",
      "Epoch 49201/50000, Loss: 0.0890, F1: 0.8709, valid F1: 0.8681\n",
      "Epoch 49301/50000, Loss: 0.0890, F1: 0.8699, valid F1: 0.8675\n",
      "Epoch 49401/50000, Loss: 0.0890, F1: 0.8704, valid F1: 0.8678\n",
      "Epoch 49501/50000, Loss: 0.0890, F1: 0.8710, valid F1: 0.8680\n",
      "Epoch 49601/50000, Loss: 0.0890, F1: 0.8708, valid F1: 0.8677\n",
      "Epoch 49701/50000, Loss: 0.0890, F1: 0.8708, valid F1: 0.8675\n",
      "Epoch 49801/50000, Loss: 0.0890, F1: 0.8704, valid F1: 0.8678\n",
      "Epoch 49901/50000, Loss: 0.0889, F1: 0.8705, valid F1: 0.8678\n",
      "[[26352  1510]\n",
      " [ 5357 22505]]\n",
      "0.8676291998380785\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(DEVICE)\n",
    "X_valid_tensor = torch.tensor(X_valid.values, dtype=torch.float32).to(DEVICE)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(DEVICE)\n",
    "y_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "input_size = X_train_tensor.shape[1]\n",
    "N_HIDDEN = 3\n",
    "BASE = 4\n",
    "hidden_size = BASE ** N_HIDDEN\n",
    "output_size = 1\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model_params = [\n",
    "    nn.Linear(input_size, hidden_size),\n",
    "]\n",
    "\n",
    "last_hidden_size = hidden_size\n",
    "for i in range(1, N_HIDDEN):\n",
    "    this_hidden_size = hidden_size // (BASE ** i)\n",
    "    model_params += [\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(last_hidden_size, this_hidden_size),\n",
    "    ]\n",
    "    last_hidden_size = this_hidden_size\n",
    "model_params += [\n",
    "    nn.Softmax(dim=output_size),\n",
    "]\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(model_params)\n",
    "\n",
    "model = nn.Sequential(*model_params).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "EPOCHS = 50000\n",
    "losses = []\n",
    "epochs = []\n",
    "accs = []\n",
    "accs_valid = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    try:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_train_tensor)[:, 1]\n",
    "        loss = loss_function(y_pred, y_train_tensor)\n",
    "        losses.append(loss.item())\n",
    "        if epoch % 100 == 0:\n",
    "            random_sample = torch.randperm(X_train_tensor.size()[0])\n",
    "            acc = f1_score(y_train_tensor.detach().cpu().numpy(), np.where(y_pred.detach().cpu() > 0.5, 1, 0))\n",
    "            accs.append(acc)\n",
    "            y_pred_valid = model(X_valid_tensor)\n",
    "            y_pred_valid = y_pred_valid.squeeze(1)[:, 1].unsqueeze(1)\n",
    "            acc_valid = f1_score(y_valid_tensor.detach().cpu().numpy(), np.where(y_pred_valid.detach().cpu() > 0.5, 1, 0))\n",
    "            accs_valid.append(acc_valid)\n",
    "            print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {loss.item():.4f}, F1: {acc:.4f}, valid F1: {acc_valid:.4f}')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Convert the valid data to PyTorch tensors and get the predicted class\n",
    "X_valid_tensor = torch.tensor(X_valid.values, dtype=torch.float32).to(DEVICE)\n",
    "y_pred_tensor = model(X_valid_tensor)[:, 1].unsqueeze(1).detach().cpu()\n",
    "\n",
    "y_pred = np.where(y_pred_tensor > 0.5, 1, 0)\n",
    "\n",
    "# _, y_pred = torch.max(y_pred_tensor, 1)\n",
    "\n",
    "# # Print the confusion matrix\n",
    "print(confusion_matrix(y_valid, y_pred))\n",
    "\n",
    "# # Print the accuracy\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_valid, y_pred))\n",
    "\n",
    "# plot the loss over the entire training procedure\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(accs, label='train')\n",
    "plt.plot(accs_valid, label='valid')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def writable(class_report, base_key = '', separator = '_'):\n",
    "    for key, value in class_report.items():\n",
    "        if isinstance(value, dict):\n",
    "            yield from writable(value, key)\n",
    "        else:\n",
    "            yield str(base_key + separator + key).replace(' ', '-'), value\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "y_pred_train = np.where(model(X_train_tensor).detach().cpu() > .5, 1, 0)[:, 1]\n",
    "y_pred_valid = np.where(model(X_valid_tensor).detach().cpu() > .5, 1, 0)[:, 1]\n",
    "y_pred_test  = np.where(model(X_test_tensor).detach().cpu()  > .5, 1, 0)[:, 1]\n",
    "\n",
    "# Confusion matrices\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axs[0].set_title('Train')\n",
    "axs[1].set_title('Valid')\n",
    "axs[2].set_title('Test')\n",
    "sns.heatmap(confusion_matrix(y_train, y_pred_train, normalize='true'), annot=True, ax=axs[0], cmap = 'Blues')\n",
    "sns.heatmap(confusion_matrix(y_valid, y_pred_valid, normalize='true'), annot=True, ax=axs[1], cmap = 'Blues')\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_test, normalize='true'), annot=True, ax=axs[2], cmap = 'Blues')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "# Scores\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "tr_cr = pd.DataFrame(writable(classification_report(y_train, y_pred_train, labels=[0, 1], output_dict=True), 'train'), columns=['0', 'train'])\n",
    "tr_cr.set_index('0', inplace=True)\n",
    "va_cr = pd.DataFrame(writable(classification_report(y_valid, y_pred_valid, labels=[0, 1], output_dict=True), 'valid'), columns=['0', 'valid'])\n",
    "va_cr.set_index('0', inplace=True)\n",
    "te_cr = pd.DataFrame(writable(classification_report(y_test, y_pred_test, labels=[0, 1], output_dict=True), 'test'), columns=['0', 'test'])\n",
    "te_cr.set_index('0', inplace=True)\n",
    "\n",
    "trvate = tr_cr.merge(va_cr, left_index=True, right_index=True)\n",
    "trvate = trvate.merge(te_cr, left_index=True, right_index=True)\n",
    "\n",
    "# remove all rows where index contains 'support'\n",
    "trvate = trvate[~trvate.index.str.contains('support')]\n",
    "\n",
    "trvate.plot.bar(ax=ax)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "y_pred_train = model(X_train_tensor)[:, 1].unsqueeze(1).detach().cpu().squeeze(1).numpy()\n",
    "y_pred_train = np.where(y_pred_train > 0.5, 1, 0)\n",
    "y_pred_valid = model(X_valid_tensor)[:, 1].unsqueeze(1).detach().cpu().squeeze(1).numpy()\n",
    "y_pred_valid = np.where(y_pred_valid > 0.5, 1, 0)\n",
    "y_pred_test = model(X_test_tensor)[:, 1].unsqueeze(1).detach().cpu().squeeze(1).numpy()\n",
    "y_pred_test = np.where(y_pred_test > 0.5, 1, 0)\n",
    "\n",
    "print(y_pred_test)\n",
    "\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy_score(y_test, y_pred_test)))\n",
    "print(\"Precision: {:.4f}\".format(precision_score(y_test, y_pred_test)))\n",
    "print(\"Recall: {:.4f}\".format(recall_score(y_test, y_pred_test)))\n",
    "print(\"F1: {:.4f}\".format(f1_score(y_test, y_pred_test)))\n",
    "print(\"ROC AUC: {:.4f}\".format(roc_auc_score(y_test, y_pred_test)))\n",
    "\n",
    "# roc curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "y_pred_train = model(X_train_tensor)[:, 1].unsqueeze(1).detach().cpu().squeeze(1).numpy()\n",
    "y_pred_valid = model(X_valid_tensor)[:, 1].unsqueeze(1).detach().cpu().squeeze(1).numpy()\n",
    "y_pred_test = model(X_test_tensor)[:, 1].unsqueeze(1).detach().cpu().squeeze(1).numpy()\n",
    "fprr, tprr, thresholdsr = roc_curve(y_train, y_pred_train)\n",
    "fprv, tprv, thresholdsv = roc_curve(y_valid, y_pred_valid)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_test)\n",
    "\n",
    "roc_aucr = auc(fprr, tprr)\n",
    "roc_aucv = auc(fprv, tprv)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(fprr, tprr, color='darkgreen', lw=2, label='ROC curve train (area = {:.4f})'.format(roc_aucr))\n",
    "plt.plot(fprv, tprv, color='darkred', lw=2, label='ROC curve valid (area = {:.4f})'.format(roc_aucv))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve test (area = {:.4f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1 - Specificity')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "\n",
    "def descale(x):\n",
    "    return scaler.inverse_transform(x)\n",
    "\n",
    "def scale(x):\n",
    "    return scaler.transform(x)\n",
    "\n",
    "explainer = LimeTabularExplainer(X_train.values, training_labels=y_train, feature_names=X_train.columns, class_names=['Not_Killed', 'Killed'], discretize_continuous=True, discretizer='entropy', verbose=True)\n",
    "\n",
    "\n",
    "# exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(x):\n",
    "    x = torch.tensor(x, dtype=torch.float32).to(DEVICE)\n",
    "    c = model(x).detach().cpu().numpy()\n",
    "    c = np.hstack([1 - c, c])\n",
    "    return c\n",
    "\n",
    "\n",
    "# exp = explainer.explain_instance(X_test.iloc[992], predict_fn, num_features=15)\n",
    "exp = explainer.explain_instance(X_test.iloc[991], predict_fn, num_features=10)\n",
    "# exp.save_to_file('exp.html')\n",
    "exp.show_in_notebook(show_table=True, show_all=False)\n",
    "exp.as_list()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "X_test_sample = X_test.sample(10)\n",
    "X_test_sample_tensor = torch.tensor(X_test_sample.values, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "e = shap.DeepExplainer(model, X_train_tensor)\n",
    "shap_values = e.shap_values(X_test_sample_tensor)\n",
    "\n",
    "shap.summary_plot(shap_values, X_test_tensor, feature_names=X_test.columns, plot_type=\"bar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
